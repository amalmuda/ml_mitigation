---
title: "Algoritme for Ã¥ identifisere avtaler relatert til utslippsreduksjon"
author: "Einar Tornes"
format:
  html:
    code-fold: true
editor: source
execute: 
  echo: true
  warning: false
  error: false
---

```{r}
#| label: load-packages

library(tidyverse)
library(tidymodels)
library(stopwords)
library(tidytext)
library(stringr)
library(textrecipes)
library(themis)
library(LiblineaR)
library(ranger)
library(doParallel)
library(noradstats)
library(here)
library(janitor)
library(knitr)

```

Let's build a model for identifying ODA agreements targeting the mitigation of climate change.

## Load data

Importing ten years of Statsys data.

```{r}
#| label: load-data

# Load statsys data for the ten last years
df_statsys <- noradstats::read_aiddata(here("data", "statsys_ten.csv")) |> 
  janitor::clean_names()

# Exclude non-ODA agreements, exclude the frame agreement level and chose years. Sorting levels.
df_oda <- df_statsys |> 
  filter(type_of_flow == "ODA",
         type_of_agreement != "Rammeavtale",
         year %in% c(2013:2017)) |> 
  mutate(pm_climate_change_mitigation = fct_relevel(pm_climate_change_mitigation, "None", after = Inf))

```

## Explore data

About ten percent of all agreement observations in the data are registered with the climate change adaptation marker as a main objective or significant objective. The rest is registered withouth such objectives.

Shifting focus to disbursed amounts, about 11 percent of disbursed amounts are to agreements with climate change mitigation as a main objective, and 3 percent to agreements with significant objective. 86 percent of disbursements are to agreements without such an objective.

```{r}
#| label: explore-mitigation

# Summarising number of observations and amounts targeting mitigation
tbl_count <- df_oda |> 
  group_by(pm_climate_change_mitigation) |> 
  summarise(
    n = n(),
    amount_nok_mill = sum(disbursed_mill_nok)) |>
  mutate(
    n_pst = n / sum(n),
    amount_pst = amount_nok_mill / sum(amount_nok_mill)) |> 
  ungroup()

knitr::kable(tbl_count)

```

Let's have a look at the most frequently used words in title and description of agreements with and without mitigation objective. "redd", "climate", "energy", "forest" and "sustainable" are among the most frequently used words in agreements targeting mitigation.

```{r explore-text}

# Making a binary variable "mitigation" and character variable "title_desc"
df_oda <- df_oda |>
  mutate(mitigation = if_else(pm_climate_change_mitigation == "None", "Not mitigation", "Mitigation")) |> 
  mutate(title_desc = paste0(agreement_title, ". ", description_of_agreement))

# Facet plot of frequently used words
df_oda |> 
  unnest_tokens(word, title_desc) |>
  anti_join(get_stopwords()) |> 
  count(mitigation, word, sort = TRUE) |>
  group_by(mitigation) |> 
  slice_max(n, n = 15) |> 
  ungroup() |> 
  mutate(word = reorder_within(word, n, mitigation)) |> 
  
  ggplot(aes(n, word, fill = mitigation)) +
  geom_col(show.legend = FALSE) +
  scale_y_reordered() +
  facet_wrap(~mitigation, scales = "free") +
  labs(x = "Word frequency",
       y = NULL) +
  theme_minimal()

```

## Build a model

We select the two relevant columns: the output variable mitigation and the predictor variable title_desc. To prevent data leakage from training to test data we include only unique observations before splitting data. There are many non-unique observations as agreements are multi-year (and sometimes have multiple observations the same year, especially statsys data).

```{r train-test}

df_oda <- df_oda |> 
  distinct(agreement_number, .keep_all = TRUE) |> 
  select(mitigation, title_desc)

# Data budget
set.seed(123)

oda_split <- df_oda |> 
  initial_split(strata = mitigation)


oda_train <- training(oda_split)
oda_test <- testing(oda_split)
  
set.seed(234)

oda_folds <- vfold_cv(oda_train, strata = mitigation)

oda_folds

```

Let's set up our recipe for preprocessing

```{r recipe-and-model}

# Preprocessing recipe. Steps for fature engineering

oda_rec <- recipe(mitigation ~ title_desc,
                  data = oda_train) |> 
  step_tokenize(title_desc) |> 
  step_stopwords(title_desc) |> 
  step_tokenfilter(title_desc, max_tokens = 1e3) |> 
  step_tfidf(title_desc) |> 
  step_normalize(all_numeric_predictors()) |> 
  step_smote(mitigation)

# These are the steps
oda_rec


# To actually do the steps we would prep(). To see what happens, bake() the recipe. There are 1001 variables. These are TF-IDF variables.
# prep(oda_rec) |> bake(new_data = NULL)
# We do not need to prep() and bake() because that happens under the hood below.
```

Let's create a model specification for each model we want to try.

```{r model-specification}

# Model spesification

rf_spec <- rand_forest(mtry = 10, trees = 300) |> 
  set_mode("classification") |> 
  set_engine("ranger")

rf_spec

svm_spec <- svm_linear() |>
  set_mode("classification") |>
  set_engine("LiblineaR")

svm_spec
```

Let's build a model workflow where we combine the data preprocessing and the model specification.

```{r workflow}
# Model workflow

rf_wf <- workflow() |> 
  add_recipe(oda_rec) |> 
  add_model(rf_spec)

rf_wf

svm_wf <- workflow() |>
  add_recipe(oda_rec) |>
  add_model(svm_spec)

svm_wf
```

```{r results}

doParallel::registerDoParallel()

set.seed(123)
rf_res <- fit_resamples(
  rf_wf,
  oda_folds,
  metrics = metric_set(accuracy, recall, precision),
  control = control_resamples(save_pred = TRUE)
)

set.seed(123)
svm_res <- fit_resamples(
  svm_wf,
  oda_folds,
  metrics = metric_set(accuracy, recall, precision),
  control = control_resamples(save_pred = TRUE)
)
```

How did the model(s) perform? Resampling results (evaluation). We can visualize these results using a confusion matrix.

```{r}
collect_metrics(rf_res)

rf_res |> 
  conf_mat_resampled(tidy = FALSE) |> 
  autoplot()

```

## Fit and evaluate final model

The function last_fit() fits one finale time on the entire training data (not only the single subsamples) and evaluates on the testing data. This is the first time we have used the testing data.

```{r}

final_res <- last_fit(
  rf_wf,
  split = oda_split,
  metrics = metric_set(accuracy, recall, precision)
)

collect_metrics(final_res) # Metrics evaluated on the testing data. No sign of overfitting.

collect_predictions(final_res) |> 
  conf_mat(mitigation, .pred_class) |> 
  autoplot()

```

The object final_res contains a fitted workflow that we can use for prediction.

```{r extract-workflow}

# Extract final fitted workflow used to train the algoritm
final_workflow <- extract_workflow(final_res)

# Predict on testing data
augment(final_workflow, new_data = oda_test[1,])

# predict(final_workflow, oda_test[1,])

```

You can save this fitted `final_wf()` object to use later with new data, for example with `readr::write_rds()`.

```{r save fitted workflow}
readr::write_rds(final_workflow, "final_workflow.rds")

```

## Deploy model

```{r}
library(vetiver)

v <- vetiver_model(final_fitted, "Climate change mitigation")

v

library(plumber)

pr() |> 
  vetiver_api(v) |> 
  pr_run()

```
