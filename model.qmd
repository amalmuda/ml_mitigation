---
title: "A model to identify ODA agreements targeting climate change mitigation"
author: "Norad - Seksjon for Statistics and analysis"
format:
  html:
    code-fold: true
editor: source
execute: 
  echo: true
  warning: false
  error: false
  enabled: false
---

Let's build a model to identify ODA agreements targeting targeting climate change mitigation.

## Load packages and data

Lets start by loading the packages used in this document.

```{r}
#| label: load-packages

library(tidyverse)
library(tidymodels)
library(stopwords)
library(tidytext)
library(stringr)
library(textrecipes)
library(themis)
library(LiblineaR)
library(ranger)
library(doParallel)
library(noradstats)
library(here)
library(janitor)
library(knitr)

```

Norwegian development aid statistics from 1960 onwards are available to download from [Access to microdata](https://resultater.norad.no/microdata). Select the the last ten years (2011-2021). You can also automatically download a csv file with microdata for the ten years 2011-2022 using [this link](https://resultater.norad.no/api/microdata?from_year=2011&to_year=2021&main_region_code=&country_iso_code=&agreement_partner_group_sid=&agreement_partner_sid=&target_area_code=&dac_main_sector_code=&dac_sub_sector_code=&chapter_code=&format=csv&language=en). Save the csv file in the *data* folder in the project directory and the file should be named *Norad-Norwegian_development_assistance*.

Let's import the csv file and only keep data for the years 2013-2018.

```{r}
#| label: load-data

df_oda <- read_csv(here("data", "Norad-Norwegian_development_assistance.csv"))

df_oda <- df_oda |> 
  filter(year %in% c(2013:2017))
```

## Build a model

The basic observation unit in the dataset are the individual agreements, identified by the unique agreement_number. Agreements can multi-year and also be financed from multiple budget lines. Therefore the basic observation unit is: agreement-by-budgetpost-by-year. An agreement can therefor be registrered multiple times in the dataset, if it is a multi-year agreement and/or the agreements is financed over multiple budget lines the same year.


**Unique agreement observations to avoid data leakage**

We want to avoid data leakage when training the model, meaning that we want to keep the training data separate from the testing data. However, this also means that we should only include unique agreement observations in the training/testing data. If not, statistical information for a multi-year agreement could be in included in both the tran set and the test set and then overestimate the models performance. The testing data would then not be really *new* data. We therefore include only one observation for each agreement in the training/testing data. For the time period 2013-2017 the total observations are X. However, when we only include unique agreement_numbers the number of unique agreement observations are redused to X.

To train the model we want to include, identified by tha variable *agreement_number*. 

**Initial data transformation: outcome variable and predictors**

- We create a dichotomous outcome variable *mitigation* based on the existing variable *policy_marker_climate_change_mitigation*. The levels *2 (main objective)* and *1 (significant objective)* are collapsed to one level *Mitigation*.
- We create a character variable containg the text information from two variables, *agreement_title* and *description_of_agreement* The character strings are collapsed into one.
- We exclude non-english characters from all character variables. This is to avoid errors using special non-english characters, like spanish letters for excample in the variable *agreement_partner*.
- We keep most of the variables in the dataset to be used as predictors in the model. We exclude all policy markers as these are often quality checked at the same time.

```{r}
#| label: data-wrangling

df_oda <- df_oda |> 
  distinct(agreement_number, .keep_all = TRUE) |> 
  mutate(mitigation = if_else(policy_marker_climate_change_mitigation == 0, "Not mitigation", "Mitigation")) |> 
  mutate(title_desc = paste0(agreement_title, ". ", description_of_agreement)) |> 
  mutate(across(where(is.character), ~str_replace_all(., pattern = "[^[\\da-zA-Z ]]", " "))) |> 
  select(c(mitigation,
           title_desc,
           agreement_partner,
           group_of_agreement_partner,
           implementing_partner,
           main_sector,
           sub_sector,
           target_area,
           recipient_country,
           recipient_region,
           type_of_assistance,
           extending_agency,
           budget_post_chapter,
           budget_post_post
           )
         )

```

## Spending our data budget

- The data split into traing set and testing set. The split is stratified on the *mitigation* variable. This ensures that our training and test data sets will keep roughly the same proportions of *mitigation* and *not relevant* agreements as in the original data.
- We create a set of 10 cross-validation resampling folds of the trainind data to evaluate the model. For each of the ten folds the model will train and evaluate.

```{r}
#| label: data-budget

set.seed(1)
oda_split <- df_oda |> 
  initial_split(strata = mitigation)


oda_train <- training(oda_split)
oda_test <- testing(oda_split)
  
set.seed(1)
oda_folds <- vfold_cv(oda_train, strata = mitigation)

#oda_folds

```

## Recipe for preprocessing

Let's set up our recipe for preprocessing.

- First, we specify the model. The *mitigation* as outcome and all other variables are predictors. We also specify the training data.

Preprocessing steps for the predictor variable *title_desc*:

- We tokenize the title_desc variable using ordinary tokenization by splitting words by space. This creates a dummy variable for each token in the variable and count the presence of each token in each "document" (title_desc).
- We remove stopwords to eliminate words that are so commonly used that they carry very little useful information, like *a*, *the* and *is*.
- We dont want to keep all the tokens, but keep the top 1000 used tokens.
- We want to weight the token (word) counts by using TF-IDF (Text Frequency-Inverse Document Frequency) for each token. TF-IDF is a weight to measure the importance of a token in the document and corpus (collection of documents).

Other preprocessing steps:

- We normalize all the numeric predictors as this is required in some models.
- We convert all factor or character variables to binary numeric variables (dummy variables).
- We transform all the nominal predictors to factors by creating dummy variables. Also we specify that unseen factor levels will be assign with a new value. Also we deal with missing (unknown) data, and we remove factor levels with zero variance.
- We handle the class imbalance in the outcome variable by using oversampling. Oversampling and undersampling can be helpful to deal with such class imbalanse to avoid poorly perfonmanse on the minority class. We oversample the "mitigation" level in the *mitigation* variable using the method *Synthetic Minority Over-sampling Technique (SMOTE)*.

```{r}
#| label: recipe

# Preprocessing recipe. Steps for fature engineering
oda_rec <- recipe(mitigation ~ ., data = oda_train) |>
  step_tokenize(title_desc) |>
  step_stopwords(title_desc) |>
  step_tokenfilter(title_desc, max_tokens = 1e3) |>
  step_tfidf(title_desc) |>
  step_normalize(all_numeric_predictors()) |>
  # step_string2factor(all_nominal()) |> 
  step_novel(all_nominal_predictors()) |>
  step_unknown(all_nominal_predictors()) |>
  step_dummy(all_nominal_predictors()) |>
  step_zv(all_predictors()) |>
  step_smote(mitigation)

#glimpse(oda_train)

# These are the steps
# oda_rec

```

Let's have a look at the training data after these preprocessing steps. The `recipe()` defines the preprocessing, the `prep()` calculates statistics from the training set, and `bake()` applies the preprocessing to data sets. These preprocessing steps are applied under the hood in the model, but it's useful to have a look to at the preprocessing.

```{r}
#| label: baked

df_train_baked <- prep(oda_rec) |> bake(new_data = NULL)

#glimpse(df_train_baked)
```

Let's create a model specification for the model(s) we want to try. We are specifying a random forest model (500 trees) and a linear support vector machines (SVM) model. 

```{r}
#| label: model-specs

rf_spec <- rand_forest(trees = 500) |> 
  set_mode("classification") |> 
  set_engine("ranger")

# rf_spec

# svm_spec <- svm_linear() |>
#   set_mode("classification") |>
#   set_engine("LiblineaR")

# svm_spec
```

## Model workflow

We build a model workflow where we put both the data recipe for preprocessing and the model specification.

```{r}
#| label: model-workflow

rf_wf <- workflow() |> 
  add_recipe(oda_rec) |> 
  add_model(rf_spec)

# rf_wf

# svm_wf <- workflow() |>
#   add_recipe(oda_rec) |>
#   add_model(svm_spec)

# svm_wf
```

## Fit a model

We fit the model workflow (preprocessing and model) on the training set (using resampling).

```{r}
#| label: resampling-results

start_time <- Sys.time()

doParallel::registerDoParallel()

set.seed(1)
rf_res <- fit_resamples(
  rf_wf,
  oda_folds,
  metrics = metric_set(accuracy, recall, precision, roc_auc, sens, spec),
  control = control_resamples(save_pred = TRUE)
)

# set.seed(1)
# svm_res <- fit_resamples(
#   svm_wf,
#   oda_folds,
#   metrics = metric_set(accuracy, recall, precision),
#   control = control_resamples(save_pred = TRUE)
# )

end_time <- Sys.time()

time <- end_time - start_time

```

## Evaluate performance

How did the model perform? Let's have a look at the resampling performance metrics. We can visualize these results using a confusion matrix.

```{r}
#| label: resampling-metrics

rf_res_metrics <- collect_metrics(rf_res)

rf_res_truefalse <- rf_res |> 
  conf_mat_resampled(tidy = FALSE)

rf_res_autoplot <- rf_res_truefalse |> 
  autoplot()

#knitr::kable(rf_res_metrics)

```

## Finalizing our model

Finally, letâ€™s make a final workflow, and then fit and evaluate the model one last time. We use the function `last_fit()` to fit the final model on the full training data set and evaluates the finalized model on the testing data set. We just need to give this funtion our original train/test data split. This is the first time we have used the testing data. The purpose of the testing data is to estimate the model performance we expect to see with new data.

```{r}
#| label: final-model

# final_res <- last_fit(
#   rf_wf,
#   split = oda_split,
#   metrics = metric_set(accuracy, recall, precision, roc_auc, sens, spec)
# )
# 
# collect_metrics(final_res) # Metrics evaluated on the testing data. No sign of overfitting.
# 
# final_res_autoplot <- collect_predictions(final_res) |> 
#   conf_mat(mitigation, .pred_class) |> 
#   autoplot()

```

The performance metrics from the test set indicate that we did not overfit during the training procedure.

## Make predictions using the workflow

The final_res object contains a finalized, fitted **workflow** that can be used for predicting on new data. We can extract this object.

```{r}
#| label: extract-workflow

# Extract final fitted workflow used to train the algoritm
#final_workflow <- extract_workflow(final_res)

# Predict on testing data
#augment(final_workflow, new_data = oda_test[1,])

```

We can save this fitted `final_wf()` object to use later with new data.

```{r save fitted workflow}

#readr::write_rds(final_workflow, "final_workflow11062020.rds")

```

## Deploy model workflow

```{r}
#| label: deploy

# library(vetiver)
# 
# v <- vetiver_model(final_fitted, "Climate change mitigation")
# 
# v
# 
# library(plumber)
# 
# pr() |> 
#   vetiver_api(v) |> 
#   pr_run()

```
